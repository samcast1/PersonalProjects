{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d773c282",
   "metadata": {},
   "source": [
    "#### Backpropogation and optimization requires us to know how far our output values are from the actual labels - since output is given as a probability distribution among classes, we need a loss function to determine the distance of each class prediction from the true value. \n",
    "\n",
    "One example for regression: \n",
    "\n",
    "Mean Absolute Error: | predicted val - target val |\n",
    "\n",
    "MAE is relatively simple. Typically, we use RMSE. \n",
    "\n",
    "For classification, loss function is generally Categorical Cross-Entropy.\n",
    "\n",
    "The categorical cross-entropy loss is defined as:\n",
    "\n",
    "$$\n",
    "L_i = - \\sum_{j} y_{i,j} \\log(\\hat{y}_{i,j})\n",
    "$$\n",
    "\n",
    "where:\n",
    "- \\( L_i \\) is the loss for the \\(i\\)-th example.\n",
    "- \\( y_{i,j} \\) is the true label (one-hot encoded) for the \\(j\\)-th class of the \\(i\\)-th example.\n",
    "- \\( \\hat{y}_{i,j} \\) is the predicted probability for the \\(j\\)-th class of the \\(i\\)-th example.\n",
    "- The summation is over all classes \\( j \\).\n",
    "\n",
    "One hot encoding ultimately reduces this to:\n",
    "\n",
    "$$\n",
    "L_i = - \\log(\\hat{y}_{i,k})\n",
    "$$\n",
    "\n",
    "where:\n",
    "- \\( L_i \\) is sample loss value.\n",
    "- \\( i \\) is i-th sample in a set.\n",
    "- \\( k \\) is target label index, index of correct class probability.\n",
    "- \\( \\hat{y} \\) represents the predicted probability of the correct class for the \\(i\\)-th sample.\n",
    "\n",
    "#### One Hot Encoding...\n",
    "\n",
    "Produces vectors to represent labels with binary values. \n",
    "\n",
    "For instance, if I have 5 classes and the label represents the 3rd class, the one hot vector may look like this:\n",
    "\n",
    "[0, 0, 1, 0, 0]\n",
    "\n",
    "#### Logarithms...\n",
    "\n",
    "Typically refers to natural logarithm:\n",
    "\n",
    "$$\n",
    "y = \\log_e x = \\ln x\n",
    "$$\n",
    "\n",
    "e is Euler's number!\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "827e19c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6486586255873816\n",
      "5.199999999999999\n"
     ]
    }
   ],
   "source": [
    "# A bit on Logs.\n",
    "# Generally speaking, a logarithm solves for x in the equation: e ^ x = b\n",
    "# So, log(b) = x\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "b = 5.2\n",
    "\n",
    "print(np.log(b))\n",
    "\n",
    "print(math.e ** 1.6486586255873816 )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31888c07",
   "metadata": {},
   "source": [
    "#### How do one hot encoding and loss function work together?\n",
    "\n",
    "Classes: 3\n",
    "Label:   0\n",
    "One-hot: [1, 0, 0]\n",
    "Prediction: [0.7, 0.1, 0.2]\n",
    "\n",
    "$$\n",
    "L_i = - \\log(\\hat{y}_{i,j}) = -(1 * \\log(0.7) + 0 * \\log(0.1) + 0 * \\log(0.2))\n",
    "$$\n",
    "\n",
    "One hot encoding means I only have one non-zero class label, and it's a one. So I only need the -log of my predicted class.\n",
    "\n",
    "This simplifies as:\n",
    "\n",
    "$$\n",
    "L_i = - \\log(\\hat{y}_{i,j} = - \\log(0.7) = 0.35667494393873245\n",
    "$$\n",
    "\n",
    "#### What does it look like to code categorical cross entropy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f9615c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.35667494393873245\n",
      "0.35667494393873245\n",
      "0.35667494393873245\n",
      "0.6931471805599453\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "\n",
    "softmax_output = [0.7, 0.1, 0.2]\n",
    "target_class = 0 # so output should be highest at index 0\n",
    "target_output = [1, 0, 0]\n",
    "\n",
    "loss = -(math.log(softmax_output[0]) * target_output[0] + # target out is 1...\n",
    "         math.log(softmax_output[1]) * target_output[1] + # zeroes out\n",
    "         math.log(softmax_output[2]) * target_output[2])  # zeroes out\n",
    "\n",
    "print(loss)\n",
    "\n",
    "loss = -math.log(softmax_output[0])\n",
    "\n",
    "print(loss)\n",
    "\n",
    "print(-math.log(0.7))\n",
    "\n",
    "# But what if the softmax output for the target class was lower?\n",
    "\n",
    "print(-math.log(0.5))\n",
    "\n",
    "# The lower the predicted value is when it should be one, the higher the -log(predicted_val), and thus the higher the loss.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3afd172d",
   "metadata": {},
   "source": [
    "#### Implementing Loss... \n",
    "\n",
    "We will need to calculate loss in batches. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c3e7557d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7 0.5 0.9]\n",
      "[0.7 0.5 0.9]\n",
      "\n",
      "Calculated loss for each batch:  [0.35667494 0.69314718 0.10536052]\n",
      "\n",
      "Average loss over all three batches:  0.38506088005216804\n"
     ]
    }
   ],
   "source": [
    "softmax_outputs = np.array([[0.7, 0.1, 0.2],\n",
    "                           [0.1, 0.5, 0.4],\n",
    "                           [0.02, 0.9, 0.08]])\n",
    "\n",
    "class_targets = [0, 1, 1] # each index represents the target class for the corresponding batch output\n",
    "\n",
    "# The goal is to grab the predicted output that corresponds to the class I was supposed to predict, i.e. the target class.\n",
    "# In the above case, each of the three batches makes predictions for three classes. \n",
    "# The first element in each vector corresponds to te first class (label = 0), so on and so forth.\n",
    "# So given the class targets, I need the first element of the first batch, the second element of the second batch, \n",
    "# and second element of the third batch. Array function helps me do this easily.\n",
    "\n",
    "print(softmax_outputs[[0,1,2], class_targets]) # softmax outputs can be indexed in vectorized form using array function\n",
    "\n",
    "# 0,1,2 represent first dimension indices. Class targets give us second dimension indices, i.e. elements. \n",
    "\n",
    "# But I don't have to hardcode the range of dimensions for the batch length! \n",
    "\n",
    "print(softmax_outputs[\n",
    "    range(len(softmax_outputs)), class_targets\n",
    "])\n",
    "\n",
    "# To calculate loss, I just need -logs...\n",
    "print()\n",
    "print(\"Calculated loss for each batch: \", -np.log(softmax_outputs[\n",
    "    range(len(softmax_outputs)), class_targets\n",
    "]))\n",
    "print()\n",
    "print(\"Average loss over all three batches: \", np.mean(-np.log(softmax_outputs[\n",
    "    range(len(softmax_outputs)), class_targets\n",
    "])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f557ef63",
   "metadata": {},
   "source": [
    "#### Finding average loss like this is great and all, but what about when I have to take -log(0)? That won't work.. Well it might, but I certainly can't grab the mean with an infinite value floating around in there.\n",
    "\n",
    "This would happen in cases where the network's confidence for the correct class is zero...pretty bad, right? Happens.\n",
    "\n",
    "To account for this, we can clip low values down to a near-zero value. To avoid some one-sided bias from clipping the low end, we clip the high end as well.\n",
    "\n",
    "Typical approach: Set range from -log(1e-7) to -log(1-1e-7).\n",
    "ypred_clipped = np.clip(ypred, 1e-7, 1 - 1e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae24888b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output probabilities:  \n",
      " [[0.33333334 0.33333334 0.33333334]\n",
      " [0.33331734 0.3333183  0.33336434]\n",
      " [0.3332888  0.33329153 0.33341965]\n",
      " [0.33325943 0.33326396 0.33347666]\n",
      " [0.33323312 0.33323926 0.33352762]]\n",
      "\n",
      "Loss:  1.098445\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import nnfs\n",
    "from nnfs.datasets import spiral_data\n",
    "\n",
    "nnfs.init()\n",
    "\n",
    "class Layer_Dense:\n",
    "    def __init__(self, n_inputs, n_neurons):      \n",
    "        self.weights = 0.10 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons)) \n",
    "    def forward(self, inputs):\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "        \n",
    "class Activation_ReLU:\n",
    "    def forward(self, inputs):\n",
    "        self.output = np.maximum(0,inputs)\n",
    "        \n",
    "class Activation_Softmax:\n",
    "    def forward(self, inputs):\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
    "        probabilities = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
    "        self.output = probabilities\n",
    "        \n",
    "class Loss: # define base class to work with various forms of loss function\n",
    "    def calculate(self, output, y):\n",
    "        sample_losses = self.forward(output, y)\n",
    "        data_loss = np.mean(sample_losses)\n",
    "        return data_loss\n",
    "        \n",
    "class Loss_CategoricalCrossentropy(Loss):\n",
    "    def forward(self, y_pred, y_true):\n",
    "        samples = len(y_pred)\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1-1e-7)\n",
    "        \n",
    "        # Need to handle both scalar inputs and one-hot encoded inputs\n",
    "        \n",
    "        if len(y_true.shape) == 1 : # indicates scalar values\n",
    "            correct_confidences = y_pred_clipped[range(samples), y_true] # grabs predicted probs that correspond to index of target vals\n",
    "        elif len(y_true.shape) == 2 : # indicates one-hot encoded vectors\n",
    "            correct_confidences = np.sum(y_pred_clipped&y_true, axis=1) # returns values within each batch that correspond to target val\n",
    "\n",
    "        negative_log_likelihoods = -np.log(correct_confidences)\n",
    "        return negative_log_likelihoods\n",
    "    \n",
    "\n",
    "\n",
    "X,y = spiral_data(samples=100, classes=3) # initialize random spiral data with 3 classes and two input features over 100 batches\n",
    "\n",
    "dense1 = Layer_Dense(2,3) # create first layer, receiving 2 inputs and passing to 3 output neurons\n",
    "activation1 = Activation_ReLU() # initialize ReLU\n",
    "\n",
    "dense2 = Layer_Dense(3,3) # create output layer, receiving 3 inputs from layer 1 and passing to 3 classes as defined in X,y above\n",
    "activation2 = Activation_Softmax() # initialize output layer activation, Softmax\n",
    "\n",
    "dense1.forward(X) # pass training data into first layer\n",
    "activation1.forward(dense1.output) # apply ReLU to clip values below zero\n",
    "\n",
    "dense2.forward(activation1.output) # pass clipped values from layer 1 to final layer\n",
    "activation2.forward(dense2.output) # apply Softmax to treat negatives and normalize range to 0,1 to produce probabilities\n",
    "\n",
    "print(\"Output probabilities: \", \"\\n\",activation2.output[:5]) # returns first 5/100 sets of 3 probabilities each, representing one prob for each class\n",
    "print()\n",
    "\n",
    "loss_function = Loss_CategoricalCrossentropy()\n",
    "loss = loss_function.calculate(activation2.output, y)\n",
    "\n",
    "print(\"Loss: \", loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9b9856",
   "metadata": {},
   "source": [
    "#### Just in case, calculate accuracy..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9fc9dd80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions:  [0 0 1]\n",
      "\n",
      "Accuracy:  0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "softmax_outputs = np.array([[0.7, 0.1, 0.2],\n",
    "                           [0.5, 0.1, 0.4], # notice that index 0 is predicted when target is index 1\n",
    "                           [0.02, 0.9, 0.08]])\n",
    "\n",
    "class_targets = [0, 1, 1]\n",
    "\n",
    "predictions = np.argmax(softmax_outputs, axis=1)\n",
    "accuracy = np.mean(predictions == class_targets)\n",
    "\n",
    "print(\"Predictions: \", predictions)\n",
    "print()\n",
    "print(\"Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f2ce91",
   "metadata": {},
   "source": [
    "#### How do we decrease loss? This is the subject of optimization!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea709d9a",
   "metadata": {},
   "source": [
    "# Unfortunately, Neural Networks from Scratch content ends here. I will move on to implement these ideas in Tensorflow and Keras, where we can continue the journey of backpropogation and optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f610fc0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
